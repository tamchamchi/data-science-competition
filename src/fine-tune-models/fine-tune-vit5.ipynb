{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9799914,"sourceType":"datasetVersion","datasetId":6006001},{"sourceId":9835687,"sourceType":"datasetVersion","datasetId":6033183}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:20:18.514037Z","iopub.execute_input":"2024-12-04T08:20:18.514540Z","iopub.status.idle":"2024-12-04T08:20:37.126647Z","shell.execute_reply.started":"2024-12-04T08:20:18.514483Z","shell.execute_reply":"2024-12-04T08:20:37.125967Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-base\")\nmodel.to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:20:37.128294Z","iopub.execute_input":"2024-12-04T08:20:37.129347Z","iopub.status.idle":"2024-12-04T08:20:43.406438Z","shell.execute_reply.started":"2024-12-04T08:20:37.129305Z","shell.execute_reply":"2024-12-04T08:20:43.405384Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff9a3ca785943bbaace9955ec00e5a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/820k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad1faa1b03443bf99194b0357c2363c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.40M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b4c887a71b4063bfbc59c30295b9af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df6d2af50fb644dab6a30b9f6e198279"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"223eeb4e47d94af68039ec10c429dc0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/904M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4284f457f2cf4fadbb6e058f69ef6cb9"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(36096, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(36096, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(36096, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=36096, bias=False)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples[\"inputs\"], max_length=1024, truncation=True, padding=True\n    )\n    \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"labels\"], max_length=256, truncation=True, padding=True\n        )\n    model_inputs['labels'] = labels['input_ids']\n    model_inputs['input_ids'] = model_inputs['input_ids']\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:20:43.407625Z","iopub.execute_input":"2024-12-04T08:20:43.408006Z","iopub.status.idle":"2024-12-04T08:20:43.413426Z","shell.execute_reply.started":"2024-12-04T08:20:43.407967Z","shell.execute_reply":"2024-12-04T08:20:43.412509Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import json\n# Đường dẫn tới file JSON\nfile_path = '/kaggle/input/data-fine-tune-vit5-v1/processed_train_demoji_remove_hashtag_url_v5.json'\n\n# Đọc dữ liệu từ JSON\nwith open(file_path, 'r', encoding='utf-8') as f:\n    data = json.load(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:20:43.415867Z","iopub.execute_input":"2024-12-04T08:20:43.416857Z","iopub.status.idle":"2024-12-04T08:20:43.559383Z","shell.execute_reply.started":"2024-12-04T08:20:43.416806Z","shell.execute_reply":"2024-12-04T08:20:43.558641Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Tạo mảng chứa các dict cho huấn luyện\ninput_lines = []\nlabel_lines = []\n\nfor key, item in data.items():\n    input_lines.append(item['caption'] + '')\n    label_lines.append(item['label'])\n    \ndict_obj = {'inputs': input_lines, 'labels': label_lines}\ndataset = Dataset.from_dict(dict_obj)\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:20:43.560350Z","iopub.execute_input":"2024-12-04T08:20:43.560650Z","iopub.status.idle":"2024-12-04T08:20:47.699197Z","shell.execute_reply.started":"2024-12-04T08:20:43.560625Z","shell.execute_reply":"2024-12-04T08:20:47.698338Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10805 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b987a89e79514645874c3b262284f6c4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import wandb\n# Khởi tạo W&B\nwandb.login()  # Bạn sẽ cần cung cấp API token để đăng nhập\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:20:47.700331Z","iopub.execute_input":"2024-12-04T08:20:47.700714Z","iopub.status.idle":"2024-12-04T08:24:07.219807Z","shell.execute_reply.started":"2024-12-04T08:20:47.700685Z","shell.execute_reply":"2024-12-04T08:24:07.218920Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n\n\ntraining_args = Seq2SeqTrainingArguments(\"tmp/\",\n                                      do_train=True,\n                                      do_eval=False,\n                                      num_train_epochs=10,\n                                      learning_rate=1e-5,\n                                      warmup_ratio=0.05,\n                                      weight_decay=0.01,\n                                      per_device_train_batch_size=4,\n                                      per_device_eval_batch_size=4,\n                                      logging_dir='./log',\n                                      group_by_length=True,\n                                      save_strategy=\"epoch\",\n                                      save_total_limit=3,\n                                      #eval_steps=1,\n                                      #evaluation_strategy=\"steps\",\n                                      # evaluation_strategy=\"no\",\n                                      fp16=True,\n                                      )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:24:07.220778Z","iopub.execute_input":"2024-12-04T08:24:07.221428Z","iopub.status.idle":"2024-12-04T08:24:07.252596Z","shell.execute_reply.started":"2024-12-04T08:24:07.221399Z","shell.execute_reply":"2024-12-04T08:24:07.252020Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets,\n    data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:24:07.253496Z","iopub.execute_input":"2024-12-04T08:24:07.253747Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnguyenduytamanh03012004\u001b[0m (\u001b[33mnguyenduytamanh03012004-uit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111369413333326, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ffbc00fb4b4183b1dcf72afe4ad95f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241204_082414-ieh498gp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nguyenduytamanh03012004-uit/huggingface/runs/ieh498gp' target=\"_blank\">tmp/</a></strong> to <a href='https://wandb.ai/nguyenduytamanh03012004-uit/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nguyenduytamanh03012004-uit/huggingface' target=\"_blank\">https://wandb.ai/nguyenduytamanh03012004-uit/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nguyenduytamanh03012004-uit/huggingface/runs/ieh498gp' target=\"_blank\">https://wandb.ai/nguyenduytamanh03012004-uit/huggingface/runs/ieh498gp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='71' max='27020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   71/27020 00:38 < 4:12:26, 1.78 it/s, Epoch 0.03/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Lưu mô hình và tokenizer vào thư mục tạm\nmodel.save_pretrained(\"./vit5-finetuned\")\ntokenizer.save_pretrained(\"./vit5-finetuned\")\n\n# Sử dụng W&B để lưu mô hình\nartifact = wandb.Artifact(\"vit5-model\", type=\"model\")\nartifact.add_dir(\"./vit5-finetuned\")\nwandb.log_artifact(artifact)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nrun = wandb.init(project=\"huggingface\")\n# Chỉ định đường dẫn đến thư mục\nartifact = wandb.Artifact(\"fine_tune_vit5\", type=\"model\")  # Đặt tên artifact và loại\n\n# Thêm thư mục vào artifact\nartifact.add_dir(\"/kaggle/working/vit5-finetuned/checkpoint-6483\")  # Đường dẫn đến thư mục của bạn\n\n# Đăng ký artifact\nrun.log_artifact(artifact)\n\n# Kết thúc run\nrun.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nrun = wandb.init()\nartifact = run.use_artifact('nguyenduytamanh03012004-uit/huggingface/fine_tune_vit5:v0', type='model')\nartifact_dir = artifact.download()\nrun.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}